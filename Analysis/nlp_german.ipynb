{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-german.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5zF44Qf1YEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "4babdc86-7460-4c94-f19d-5364df82c7e5"
      },
      "source": [
        "!pip install spacy \n",
        "!python -m spacy download de"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Collecting de_core_news_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 4.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.1.0-cp36-none-any.whl size=11073065 sha256=696ee5ce36eec2c2123830b7ca386ec6569b8d06fd24017a05e253a75a97f5a3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5z9nywlw/wheels/b4/8b/5e/d2ce5d2756ca95de22f50f68299708009a4aafda2aea79c4e4\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C528Dd401k8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "text = 'Juliana kommt aus Paris. Das ist die Hauptstadt von Frankreich. In diesem Sommer macht sie einen Sprachkurs in Freiburg. Das ist eine Universitätsstadt im Süden von Deutschland.'\n",
        "\n",
        "# load german models\n",
        "nlp = spacy.load(\"de\")\n",
        "\n",
        "# annotate text\n",
        "tokens = nlp(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SoYqcZL1opi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igPfe3uv1o_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "6c859581-66e3-4bce-cceb-6954ef569339"
      },
      "source": [
        "df = pd.DataFrame(columns = [\"Token\",\"Lemma\",\"POS\",\t\"Tag\",\t\"Dep\",\t\"Shape\",\t\"alpha\"\t,\"stop\"])\n",
        "for token in tokens:\n",
        "    df = df.append(pd.Series([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop], index=df.columns ), ignore_index=True)\n",
        "df.head(20)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>Lemma</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>Dep</th>\n",
              "      <th>Shape</th>\n",
              "      <th>alpha</th>\n",
              "      <th>stop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Juliana</td>\n",
              "      <td>Juliana</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NE</td>\n",
              "      <td>sb</td>\n",
              "      <td>Xxxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>kommt</td>\n",
              "      <td>kommen</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VVFIN</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aus</td>\n",
              "      <td>aus</td>\n",
              "      <td>ADP</td>\n",
              "      <td>APPR</td>\n",
              "      <td>mo</td>\n",
              "      <td>xxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Paris</td>\n",
              "      <td>Paris</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NE</td>\n",
              "      <td>nk</td>\n",
              "      <td>Xxxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>$.</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Das</td>\n",
              "      <td>der</td>\n",
              "      <td>PRON</td>\n",
              "      <td>PDS</td>\n",
              "      <td>sb</td>\n",
              "      <td>Xxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ist</td>\n",
              "      <td>sein</td>\n",
              "      <td>AUX</td>\n",
              "      <td>VAFIN</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>xxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>die</td>\n",
              "      <td>der</td>\n",
              "      <td>DET</td>\n",
              "      <td>ART</td>\n",
              "      <td>nk</td>\n",
              "      <td>xxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Hauptstadt</td>\n",
              "      <td>Hauptstadt</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NN</td>\n",
              "      <td>pd</td>\n",
              "      <td>Xxxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>von</td>\n",
              "      <td>von</td>\n",
              "      <td>ADP</td>\n",
              "      <td>APPR</td>\n",
              "      <td>pg</td>\n",
              "      <td>xxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Frankreich</td>\n",
              "      <td>Frankreich</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NE</td>\n",
              "      <td>nk</td>\n",
              "      <td>Xxxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>$.</td>\n",
              "      <td>punct</td>\n",
              "      <td>.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>In</td>\n",
              "      <td>In</td>\n",
              "      <td>ADP</td>\n",
              "      <td>APPR</td>\n",
              "      <td>mo</td>\n",
              "      <td>Xx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>diesem</td>\n",
              "      <td>dies</td>\n",
              "      <td>DET</td>\n",
              "      <td>PDAT</td>\n",
              "      <td>nk</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Sommer</td>\n",
              "      <td>Sommer</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NN</td>\n",
              "      <td>nk</td>\n",
              "      <td>Xxxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>macht</td>\n",
              "      <td>machen</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VVFIN</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>sie</td>\n",
              "      <td>ich</td>\n",
              "      <td>PRON</td>\n",
              "      <td>PPER</td>\n",
              "      <td>sb</td>\n",
              "      <td>xxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>einen</td>\n",
              "      <td>ein</td>\n",
              "      <td>DET</td>\n",
              "      <td>ART</td>\n",
              "      <td>nk</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Sprachkurs</td>\n",
              "      <td>Sprachkurs</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NN</td>\n",
              "      <td>oa</td>\n",
              "      <td>Xxxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>ADP</td>\n",
              "      <td>APPR</td>\n",
              "      <td>mnr</td>\n",
              "      <td>xx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Token       Lemma    POS    Tag    Dep  Shape  alpha   stop\n",
              "0      Juliana     Juliana  PROPN     NE     sb  Xxxxx   True  False\n",
              "1        kommt      kommen   VERB  VVFIN   ROOT   xxxx   True   True\n",
              "2          aus         aus    ADP   APPR     mo    xxx   True   True\n",
              "3        Paris       Paris  PROPN     NE     nk  Xxxxx   True  False\n",
              "4            .           .  PUNCT     $.  punct      .  False  False\n",
              "5          Das         der   PRON    PDS     sb    Xxx   True   True\n",
              "6          ist        sein    AUX  VAFIN   ROOT    xxx   True   True\n",
              "7          die         der    DET    ART     nk    xxx   True   True\n",
              "8   Hauptstadt  Hauptstadt   NOUN     NN     pd  Xxxxx   True  False\n",
              "9          von         von    ADP   APPR     pg    xxx   True   True\n",
              "10  Frankreich  Frankreich  PROPN     NE     nk  Xxxxx   True  False\n",
              "11           .           .  PUNCT     $.  punct      .  False  False\n",
              "12          In          In    ADP   APPR     mo     Xx   True   True\n",
              "13      diesem        dies    DET   PDAT     nk   xxxx   True   True\n",
              "14      Sommer      Sommer   NOUN     NN     nk  Xxxxx   True  False\n",
              "15       macht      machen   VERB  VVFIN   ROOT   xxxx   True   True\n",
              "16         sie         ich   PRON   PPER     sb    xxx   True   True\n",
              "17       einen         ein    DET    ART     nk   xxxx   True   True\n",
              "18  Sprachkurs  Sprachkurs   NOUN     NN     oa  Xxxxx   True  False\n",
              "19          in          in    ADP   APPR    mnr     xx   True   True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5jtq6oR5_of",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad23ff66-8a06-4ec5-f8c2-78f066dc89b6"
      },
      "source": [
        "from textblob_de import TextBlobDE\n",
        "\n",
        "text = 'I am happy'\n",
        "\n",
        "blob = TextBlobDE(text)\n",
        "print(TextBlobDE(text).sentiment.polarity)\n",
        "# for sentence in blob.sentences:\n",
        "#     print(sentence.sentiment.polarity)\n",
        "# # 1.0\n",
        "# 0.0\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2_-FVuKpb37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "870beb3a-3539-425e-96c3-839dc4be8185"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = 'I am very happy'\n",
        "\n",
        "# blob = TextBlobDE(text)\n",
        "print(TextBlob(text).sentiment.polarity)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eDI_1M6AH71",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "eb9e3747-e018-4349-bf67-48bf6866c99c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj5wecaa_WHK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "12538f60-0e7a-4b37-cc53-a476d1aa04dc"
      },
      "source": [
        "!pip install textblob-de"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textblob-de\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/61/7a5759c3ac60bf9330a50ce81ebe7f0aac1bc6c674d45e00f7b3e190f5af/textblob_de-0.4.3-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: textblob>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from textblob-de) (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob>=0.9.0->textblob-de) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob>=0.9.0->textblob-de) (1.12.0)\n",
            "Installing collected packages: textblob-de\n",
            "Successfully installed textblob-de-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9TcWJMg7_yB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-BRhnJKFwTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def frames(data):\n",
        "  data = pd.DataFrame(data.values, columns = [\"word\",\"POS\",\"polarity\",\"inf\"] )\n",
        "  data1 = data.copy()\n",
        "  data1 = data1.drop(['inf'], axis=1)\n",
        "  data = data.assign(inf=data['inf'].str.split(',')).explode('inf')\n",
        "  data2 = data[['inf','POS','polarity']]\n",
        "  data2 = data2.rename(columns={'inf': 'word'})\n",
        "  data3 = data1.append(data2,ignore_index=True,sort=True)\n",
        "  data3 = data3.dropna()\n",
        "  return data3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE8-8fgOTnqG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "157c46aa-118b-4d14-ec2c-169dde019c82"
      },
      "source": [
        "neg_data = pd.read_csv('SentiWS_v1_8c_Negative.txt', sep=r'[\\t|]', header=None, engine='python')\n",
        "pos_data = pd.read_csv('SentiWS_v1_8c_Positive.txt', sep=r'[\\t|]', header=None, engine='python')\n",
        "neg_data = frames(neg_data)\n",
        "pos_data = frames(pos_data)\n",
        "corpus = pos_data.append(neg_data,ignore_index=True,sort=True)\n",
        "print(corpus.shape)\n",
        "corpus.head()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31281, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>POS</th>\n",
              "      <th>polarity</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NN</td>\n",
              "      <td>0.004</td>\n",
              "      <td>Abmachung</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NN</td>\n",
              "      <td>0.004</td>\n",
              "      <td>Abschluß</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NN</td>\n",
              "      <td>0.004</td>\n",
              "      <td>Abstimmung</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NN</td>\n",
              "      <td>0.004</td>\n",
              "      <td>Agilität</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NN</td>\n",
              "      <td>0.004</td>\n",
              "      <td>Aktivität</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  POS polarity        word\n",
              "0  NN    0.004   Abmachung\n",
              "1  NN    0.004    Abschluß\n",
              "2  NN    0.004  Abstimmung\n",
              "3  NN    0.004    Agilität\n",
              "4  NN    0.004   Aktivität"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FECch7gfJrd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "46ede0cd-add1-4ad1-f995-f63a11e090a9"
      },
      "source": [
        "from textblob_de import TextBlobDE\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "text = u'Voraussetzung für die Einschreibung ist der Nachweis des Bachelorabschlusses. Wenn Ihr Bachelorabschluss bereits vorliegt, können Sie sich zum oben genannten Semester einschreiben. Wenn Ihr Bachelorabschluss jetzt noch nicht vorliegt, können Sie sich noch nicht einschreiben, sondern erst dann, wenn Sie den Abschluss erlangt haben. Diese bedingte Zulassung gilt für das Wintersemester 2020/21 sowie für die beiden darauf folgenden Semester. Die Einschreibung in das Sommersemester ist bis 15.05. und in das Wintersemester bis 15.11. möglich. Sie können sich ab dem 01. März in das Sommersemester und ab dem 01. September in das Wintersemester einschreiben. Wenn Sie vor dieser Bewerbung bereits studiert haben, steht die Zulassung unter dem Vorbehalt, dass Sie den Prüfungsanspruch in obigem Studiengang nicht verloren haben, was durch den Prüfungsausschuss im Rahmen der Einschreibung festzustellen ist. Nur für Bewerbungen auf Basis einer früheren bedingten Zulassung mit Auflagen gilt: Die erteilten Auflagen gelten weiterhin.Wenn Ihr Bachelorabschluss jetzt noch nicht vorliegt, können Sie sich noch nicht einschreiben, sondern erst dann, wenn Sie den Abschluss erlangt haben. Diese bedingte Zulassung gilt für das Wintersemester 2020/21 sowie für die beiden darauf folgenden Semester.Wenn Sie vor dieser Bewerbung bereits studiert haben, steht die Zulassung unter dem Vorbehalt, dass Sie den Prüfungsanspruch in obigem Studiengang nicht verloren haben, was durch den Prüfungsausschuss im Rahmen der Einschreibung festzustellen ist. '\n",
        "token = nlp(text)\n",
        "\n",
        "# load german models\n",
        "nlp = spacy.load(\"de\")\n",
        "\n",
        "# annotate text\n",
        "tokens = nlp(text)\n",
        "df = pd.DataFrame(columns = [\"Token\",\"Lemma\",\"POS\",\t\"Tag\",\t\"Dep\",\t\"Shape\",\t\"alpha\"\t,\"stop\"])\n",
        "token_list = []\n",
        "lemma_list = []\n",
        "for token in tokens:\n",
        "    token_list.append(token.text)\n",
        "    lemma_list.append(token.lemma_)\n",
        "    df = df.append(pd.Series([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop], index=df.columns ), ignore_index=True)\n",
        "print(df)\n",
        "\n",
        "from spacy.lang.de.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[] \n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word) \n",
        "print(token_list)\n",
        "print(lemma_list)\n",
        "print(filtered_sentence)   \n",
        "k = list(filter(lambda a: a != '.', filtered_sentence))\n",
        "print(k)\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             Token          Lemma    POS    Tag    Dep  Shape  alpha   stop\n",
            "0    Voraussetzung  Voraussetzung   NOUN     NN     pd  Xxxxx   True  False\n",
            "1              für            für    ADP   APPR    mnr    xxx   True   True\n",
            "2              die            der    DET    ART     nk    xxx   True   True\n",
            "3    Einschreibung  Einschreibung   NOUN     NN     nk  Xxxxx   True  False\n",
            "4              ist           sein    AUX  VAFIN   ROOT    xxx   True   True\n",
            "..             ...            ...    ...    ...    ...    ...    ...    ...\n",
            "239            der            der    DET    ART     nk    xxx   True   True\n",
            "240  Einschreibung  Einschreibung   NOUN     NN     ag  Xxxxx   True  False\n",
            "241  festzustellen    feststellen   VERB  VVIZU     oc   xxxx   True  False\n",
            "242            ist           sein    AUX  VAFIN     rc    xxx   True   True\n",
            "243              .              .  PUNCT     $.  punct      .  False  False\n",
            "\n",
            "[244 rows x 8 columns]\n",
            "['Voraussetzung', 'für', 'die', 'Einschreibung', 'ist', 'der', 'Nachweis', 'des', 'Bachelorabschlusses', '.', 'Wenn', 'Ihr', 'Bachelorabschluss', 'bereits', 'vorliegt', ',', 'können', 'Sie', 'sich', 'zum', 'oben', 'genannten', 'Semester', 'einschreiben', '.', 'Wenn', 'Ihr', 'Bachelorabschluss', 'jetzt', 'noch', 'nicht', 'vorliegt', ',', 'können', 'Sie', 'sich', 'noch', 'nicht', 'einschreiben', ',', 'sondern', 'erst', 'dann', ',', 'wenn', 'Sie', 'den', 'Abschluss', 'erlangt', 'haben', '.', 'Diese', 'bedingte', 'Zulassung', 'gilt', 'für', 'das', 'Wintersemester', '2020/21', 'sowie', 'für', 'die', 'beiden', 'darauf', 'folgenden', 'Semester', '.', 'Die', 'Einschreibung', 'in', 'das', 'Sommersemester', 'ist', 'bis', '15.05', '.', 'und', 'in', 'das', 'Wintersemester', 'bis', '15.11', '.', 'möglich', '.', 'Sie', 'können', 'sich', 'ab', 'dem', '01', '.', 'März', 'in', 'das', 'Sommersemester', 'und', 'ab', 'dem', '01', '.', 'September', 'in', 'das', 'Wintersemester', 'einschreiben', '.', 'Wenn', 'Sie', 'vor', 'dieser', 'Bewerbung', 'bereits', 'studiert', 'haben', ',', 'steht', 'die', 'Zulassung', 'unter', 'dem', 'Vorbehalt', ',', 'dass', 'Sie', 'den', 'Prüfungsanspruch', 'in', 'obigem', 'Studiengang', 'nicht', 'verloren', 'haben', ',', 'was', 'durch', 'den', 'Prüfungsausschuss', 'im', 'Rahmen', 'der', 'Einschreibung', 'festzustellen', 'ist', '.', 'Nur', 'für', 'Bewerbungen', 'auf', 'Basis', 'einer', 'früheren', 'bedingten', 'Zulassung', 'mit', 'Auflagen', 'gilt', ':', 'Die', 'erteilten', 'Auflagen', 'gelten', 'weiterhin', '.', 'Wenn', 'Ihr', 'Bachelorabschluss', 'jetzt', 'noch', 'nicht', 'vorliegt', ',', 'können', 'Sie', 'sich', 'noch', 'nicht', 'einschreiben', ',', 'sondern', 'erst', 'dann', ',', 'wenn', 'Sie', 'den', 'Abschluss', 'erlangt', 'haben', '.', 'Diese', 'bedingte', 'Zulassung', 'gilt', 'für', 'das', 'Wintersemester', '2020/21', 'sowie', 'für', 'die', 'beiden', 'darauf', 'folgenden', 'Semester', '.', 'Wenn', 'Sie', 'vor', 'dieser', 'Bewerbung', 'bereits', 'studiert', 'haben', ',', 'steht', 'die', 'Zulassung', 'unter', 'dem', 'Vorbehalt', ',', 'dass', 'Sie', 'den', 'Prüfungsanspruch', 'in', 'obigem', 'Studiengang', 'nicht', 'verloren', 'haben', ',', 'was', 'durch', 'den', 'Prüfungsausschuss', 'im', 'Rahmen', 'der', 'Einschreibung', 'festzustellen', 'ist', '.']\n",
            "['Voraussetzung', 'für', 'der', 'Einschreibung', 'sein', 'der', 'Nachweis', 'der', 'Bachelorabschlusses', '.', 'wenn', 'mein', 'Bachelorabschluss', 'bereits', 'vorliegen', ',', 'können', 'ich', 'sich', 'zum', 'oben', 'genannt', 'Semester', 'einschreiben', '.', 'wenn', 'mein', 'Bachelorabschluss', 'jetzt', 'noch', 'nicht', 'vorliegen', ',', 'können', 'ich', 'sich', 'noch', 'nicht', 'einschreiben', ',', 'sondern', 'erst', 'dann', ',', 'wenn', 'ich', 'der', 'Abschluss', 'erlangen', 'haben', '.', 'Diese', 'bedingte', 'Zulassung', 'gelten', 'für', 'der', 'Wintersemester', '2020/21', 'sowie', 'für', 'der', 'beid', 'darauf', 'folgend', 'Semester', '.', 'der', 'Einschreibung', 'in', 'der', 'Sommersemester', 'sein', 'bis', '15.05', '.', 'und', 'in', 'der', 'Wintersemester', 'bis', '15.11', '.', 'möglich', '.', 'ich', 'können', 'sich', 'ab', 'der', '01', '.', 'März', 'in', 'der', 'Sommersemester', 'und', 'ab', 'der', '01', '.', 'September', 'in', 'der', 'Wintersemester', 'einschreiben', '.', 'wenn', 'ich', 'vor', 'dies', 'Bewerbung', 'bereits', 'studieren', 'haben', ',', 'stehen', 'der', 'Zulassung', 'unter', 'der', 'Vorbehalt', ',', 'dass', 'ich', 'der', 'Prüfungsanspruch', 'in', 'obig', 'Studiengang', 'nicht', 'verlieren', 'haben', ',', 'was', 'durch', 'der', 'Prüfungsausschuss', 'im', 'Rahmen', 'der', 'Einschreibung', 'feststellen', 'sein', '.', 'Nur', 'für', 'Bewerbung', 'auf', 'Basis', 'einer', 'früh', 'bedingt', 'Zulassung', 'mit', 'aufliegen', 'gelten', ':', 'der', 'erteilen', 'aufliegen', 'gelten', 'weiterhin', '.', 'wenn', 'mein', 'Bachelorabschluss', 'jetzt', 'noch', 'nicht', 'vorliegen', ',', 'können', 'ich', 'sich', 'noch', 'nicht', 'einschreiben', ',', 'sondern', 'erst', 'dann', ',', 'wenn', 'ich', 'der', 'Abschluss', 'erlangen', 'haben', '.', 'Diese', 'bedingte', 'Zulassung', 'gelten', 'für', 'der', 'Wintersemester', '2020/21', 'sowie', 'für', 'der', 'beid', 'darauf', 'folgend', 'Semester', '.', 'wenn', 'ich', 'vor', 'dies', 'Bewerbung', 'bereits', 'studieren', 'haben', ',', 'stehen', 'der', 'Zulassung', 'unter', 'der', 'Vorbehalt', ',', 'dass', 'ich', 'der', 'Prüfungsanspruch', 'in', 'obig', 'Studiengang', 'nicht', 'verlieren', 'haben', ',', 'was', 'durch', 'der', 'Prüfungsausschuss', 'im', 'Rahmen', 'der', 'Einschreibung', 'feststellen', 'sein', '.']\n",
            "['Voraussetzung', 'Einschreibung', 'Nachweis', 'Bachelorabschlusses', '.', 'Bachelorabschluss', 'vorliegt', ',', 'genannten', 'Semester', 'einschreiben', '.', 'Bachelorabschluss', 'vorliegt', ',', 'einschreiben', ',', ',', 'Abschluss', 'erlangt', '.', 'bedingte', 'Zulassung', 'gilt', 'Wintersemester', '2020/21', 'folgenden', 'Semester', '.', 'Einschreibung', 'Sommersemester', '15.05', '.', 'Wintersemester', '15.11', '.', '.', '01', '.', 'März', 'Sommersemester', '01', '.', 'September', 'Wintersemester', 'einschreiben', '.', 'Bewerbung', 'studiert', ',', 'steht', 'Zulassung', 'Vorbehalt', ',', 'Prüfungsanspruch', 'obigem', 'Studiengang', 'verloren', ',', 'Prüfungsausschuss', 'Rahmen', 'Einschreibung', 'festzustellen', '.', 'Bewerbungen', 'Basis', 'früheren', 'bedingten', 'Zulassung', 'Auflagen', 'gilt', ':', 'erteilten', 'Auflagen', 'gelten', 'weiterhin', '.', 'Bachelorabschluss', 'vorliegt', ',', 'einschreiben', ',', ',', 'Abschluss', 'erlangt', '.', 'bedingte', 'Zulassung', 'gilt', 'Wintersemester', '2020/21', 'folgenden', 'Semester', '.', 'Bewerbung', 'studiert', ',', 'steht', 'Zulassung', 'Vorbehalt', ',', 'Prüfungsanspruch', 'obigem', 'Studiengang', 'verloren', ',', 'Prüfungsausschuss', 'Rahmen', 'Einschreibung', 'festzustellen', '.']\n",
            "['Voraussetzung', 'Einschreibung', 'Nachweis', 'Bachelorabschlusses', 'Bachelorabschluss', 'vorliegt', ',', 'genannten', 'Semester', 'einschreiben', 'Bachelorabschluss', 'vorliegt', ',', 'einschreiben', ',', ',', 'Abschluss', 'erlangt', 'bedingte', 'Zulassung', 'gilt', 'Wintersemester', '2020/21', 'folgenden', 'Semester', 'Einschreibung', 'Sommersemester', '15.05', 'Wintersemester', '15.11', '01', 'März', 'Sommersemester', '01', 'September', 'Wintersemester', 'einschreiben', 'Bewerbung', 'studiert', ',', 'steht', 'Zulassung', 'Vorbehalt', ',', 'Prüfungsanspruch', 'obigem', 'Studiengang', 'verloren', ',', 'Prüfungsausschuss', 'Rahmen', 'Einschreibung', 'festzustellen', 'Bewerbungen', 'Basis', 'früheren', 'bedingten', 'Zulassung', 'Auflagen', 'gilt', ':', 'erteilten', 'Auflagen', 'gelten', 'weiterhin', 'Bachelorabschluss', 'vorliegt', ',', 'einschreiben', ',', ',', 'Abschluss', 'erlangt', 'bedingte', 'Zulassung', 'gilt', 'Wintersemester', '2020/21', 'folgenden', 'Semester', 'Bewerbung', 'studiert', ',', 'steht', 'Zulassung', 'Vorbehalt', ',', 'Prüfungsanspruch', 'obigem', 'Studiengang', 'verloren', ',', 'Prüfungsausschuss', 'Rahmen', 'Einschreibung', 'festzustellen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0aNH1nnd9BI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d925f03-9e71-4bec-9595-5f9fc1e0f6cb"
      },
      "source": [
        "for i in lemma_list:\n",
        "  print(corpus.index[corpus['word'] == i].tolist())"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[1191]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[17312]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[17312]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4zGGf9hFwjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "3520ef71-0f47-4a06-b9fd-a76570d9e7df"
      },
      "source": [
        "data3.tail(100)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>POS</th>\n",
              "      <th>polarity</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15931</th>\n",
              "      <td>ADJX</td>\n",
              "      <td>-0.5154</td>\n",
              "      <td>überflüssiges</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15932</th>\n",
              "      <td>ADJX</td>\n",
              "      <td>-0.5154</td>\n",
              "      <td>überflüssiger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15933</th>\n",
              "      <td>ADJX</td>\n",
              "      <td>-0.5154</td>\n",
              "      <td>überflüssigere</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15934</th>\n",
              "      <td>ADJX</td>\n",
              "      <td>-0.5154</td>\n",
              "      <td>überflüssigerem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15935</th>\n",
              "      <td>ADJX</td>\n",
              "      <td>-0.5154</td>\n",
              "      <td>überflüssigstes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16031</th>\n",
              "      <td>VVINF</td>\n",
              "      <td>-0.0048</td>\n",
              "      <td>überwältigtest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16032</th>\n",
              "      <td>VVINF</td>\n",
              "      <td>-0.0048</td>\n",
              "      <td>überwältigst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16033</th>\n",
              "      <td>VVINF</td>\n",
              "      <td>-0.0048</td>\n",
              "      <td>überwältiget</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16034</th>\n",
              "      <td>VVINF</td>\n",
              "      <td>-0.0048</td>\n",
              "      <td>überwältig</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16035</th>\n",
              "      <td>VVINF</td>\n",
              "      <td>-0.0048</td>\n",
              "      <td>überwältigtet</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         POS polarity             word\n",
              "15931   ADJX  -0.5154    überflüssiges\n",
              "15932   ADJX  -0.5154    überflüssiger\n",
              "15933   ADJX  -0.5154   überflüssigere\n",
              "15934   ADJX  -0.5154  überflüssigerem\n",
              "15935   ADJX  -0.5154  überflüssigstes\n",
              "...      ...      ...              ...\n",
              "16031  VVINF  -0.0048   überwältigtest\n",
              "16032  VVINF  -0.0048     überwältigst\n",
              "16033  VVINF  -0.0048     überwältiget\n",
              "16034  VVINF  -0.0048       überwältig\n",
              "16035  VVINF  -0.0048    überwältigtet\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-6e5nR6Fwn3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b407fc6d-b849-464d-959c-9fa121710a12"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc_OZ9pA1thF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path = r'/content/drive/My Drive/aw' \n",
        "all_files = glob.glob(path + \"/*.csv\")\n",
        "li = []\n",
        "\n",
        "for filename in all_files:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)\n",
        "    li.append(df)\n",
        "\n",
        "frame = pd.concat(li, axis=0, ignore_index=True, sort=False)\n",
        "frame = frame.drop([\"web-scraper-order\",\"web-scraper-start-url\",\"author\",\"date\",\"next\",\"next-href\",\"Unnamed: 9\"],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1eitelr1t4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0e10fdae-f2f2-4824-cf08-4c51d4139be8"
      },
      "source": [
        "frame.head()"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Das Beste iPhone aller Zeiten</td>\n",
              "      <td>Ich bin sehr zufrieden mit dem iPhone 11. Der ...</td>\n",
              "      <td>5,0 von 5 Sternen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>besser als beim hersteller</td>\n",
              "      <td>gestern bestellt, heute geliefert. besser geht...</td>\n",
              "      <td>5,0 von 5 Sternen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Gutes Handy mit kleinen Schwächen</td>\n",
              "      <td>Ich mach es mal kurz:\\nGut: Optik, Verarbeitun...</td>\n",
              "      <td>4,0 von 5 Sternen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ein sehr edles Stück dieses IPHONE 11</td>\n",
              "      <td>Amazon hat wieder super-schnell geliefert. Dan...</td>\n",
              "      <td>5,0 von 5 Sternen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Super</td>\n",
              "      <td>Viel früher angekommen als angegeben, tolles H...</td>\n",
              "      <td>5,0 von 5 Sternen</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   title  ...             rating\n",
              "0          Das Beste iPhone aller Zeiten  ...  5,0 von 5 Sternen\n",
              "1             besser als beim hersteller  ...  5,0 von 5 Sternen\n",
              "2      Gutes Handy mit kleinen Schwächen  ...  4,0 von 5 Sternen\n",
              "3  Ein sehr edles Stück dieses IPHONE 11  ...  5,0 von 5 Sternen\n",
              "4                                  Super  ...  5,0 von 5 Sternen\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQUEElKcA6Dm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ca0566e0-4a72-46d5-bfe0-318fd11bd551"
      },
      "source": [
        "frame = frame.replace(\"5,0 von 5 Sternen\",5)\n",
        "frame = frame.replace(\"4,0 von 5 Sternen\",4)\n",
        "frame = frame.replace(\"3,0 von 5 Sternen\",3)\n",
        "frame = frame.replace(\"2,0 von 5 Sternen\",2)\n",
        "frame = frame.replace(\"1,0 von 5 Sternen\",1)\n",
        "\n",
        "frame.head()"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Das Beste iPhone aller Zeiten</td>\n",
              "      <td>Ich bin sehr zufrieden mit dem iPhone 11. Der ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>besser als beim hersteller</td>\n",
              "      <td>gestern bestellt, heute geliefert. besser geht...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Gutes Handy mit kleinen Schwächen</td>\n",
              "      <td>Ich mach es mal kurz:\\nGut: Optik, Verarbeitun...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ein sehr edles Stück dieses IPHONE 11</td>\n",
              "      <td>Amazon hat wieder super-schnell geliefert. Dan...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Super</td>\n",
              "      <td>Viel früher angekommen als angegeben, tolles H...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   title  ... rating\n",
              "0          Das Beste iPhone aller Zeiten  ...    5.0\n",
              "1             besser als beim hersteller  ...    5.0\n",
              "2      Gutes Handy mit kleinen Schwächen  ...    4.0\n",
              "3  Ein sehr edles Stück dieses IPHONE 11  ...    5.0\n",
              "4                                  Super  ...    5.0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxbbAQeMEI3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "01d3f5d2-845c-470e-d469-a1f97af978ce"
      },
      "source": [
        "l1 =[]\n",
        "for i in frame.rating:\n",
        "  if i>=3:\n",
        "    l1.append(1)\n",
        "  else:\n",
        "    l1.append(0)\n",
        "frame[\"feedback\"] = pd.DataFrame(l1)\n",
        "frame.head()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>rating</th>\n",
              "      <th>feedback</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Das Beste iPhone aller Zeiten</td>\n",
              "      <td>Ich bin sehr zufrieden mit dem iPhone 11. Der ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>besser als beim hersteller</td>\n",
              "      <td>gestern bestellt, heute geliefert. besser geht...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Gutes Handy mit kleinen Schwächen</td>\n",
              "      <td>Ich mach es mal kurz:\\nGut: Optik, Verarbeitun...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ein sehr edles Stück dieses IPHONE 11</td>\n",
              "      <td>Amazon hat wieder super-schnell geliefert. Dan...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Super</td>\n",
              "      <td>Viel früher angekommen als angegeben, tolles H...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   title  ... feedback\n",
              "0          Das Beste iPhone aller Zeiten  ...        1\n",
              "1             besser als beim hersteller  ...        1\n",
              "2      Gutes Handy mit kleinen Schwächen  ...        1\n",
              "3  Ein sehr edles Stück dieses IPHONE 11  ...        1\n",
              "4                                  Super  ...        1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck99_C-2FGlY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "220a40cb-595e-4e37-e7cc-0b72c7afcfef"
      },
      "source": [
        "\n",
        "frame['title'] = frame['title'].astype(str)\n",
        "frame['content'] = frame['content'].astype(str)\n",
        "frame.info()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3413 entries, 0 to 3412\n",
            "Data columns (total 4 columns):\n",
            "title       3413 non-null object\n",
            "content     3413 non-null object\n",
            "rating      3409 non-null float64\n",
            "feedback    3413 non-null int64\n",
            "dtypes: float64(1), int64(1), object(2)\n",
            "memory usage: 106.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmTyUf2XFyYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89V15ytjF1Li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import spacy\n",
        "from spacy.lang.de.stop_words import STOP_WORDS\n",
        "from spacy.lang.de import German\n",
        "\n",
        "punctuations = string.punctuation\n",
        "\n",
        "nlp = spacy.load(\"de\")\n",
        "stop_words = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "parser = German()\n",
        "\n",
        "def spacy_tokenizer(sentence):\n",
        "    \n",
        "    mytokens = parser(sentence)\n",
        "    \n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "    \n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "    \n",
        "    return mytokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr9jeJpOSYQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b752b544-6bbe-4c47-8c42-34e14d11d41f"
      },
      "source": [
        "print(spacy_tokenizer(\" Material ist unfaßbar schlecht!!! \"))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['material', 'unfaßbar']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nasf_alPHgt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom transformer using spaCy\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    return text.strip().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnU7189nF1c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwy3fF9JF1pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6DkwAOIF1sP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = frame['content'] \n",
        "ylabels = frame['feedback'] \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-31XOBWNF1w8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "c4591ea6-051e-4dc4-f44b-5d645deb68e8"
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe_log = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe_log.fit(X_train,y_train)"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('cleaner', <__main__.predictors object at 0x7f7f63b0e3c8>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 t...\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function spacy_tokenizer at 0x7f7f6907eea0>,\n",
              "                                 vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsZmzikbMoXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "9a814bb1-2514-4e04-825e-221d85f0fc9a"
      },
      "source": [
        "# SVM classifier\n",
        "from sklearn import svm\n",
        "classifier = svm.SVC(kernel='linear')\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe_svm = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe_svm.fit(X_train,y_train)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('cleaner', <__main__.predictors object at 0x7f7f6b4a0080>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 2), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 t...rn='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function spacy_tokenizer at 0x7f7f6907e9d8>,\n",
              "                                 vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='linear', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahUA1mKDROfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7e5b681a-314e-4138-fc0f-189d53402304"
      },
      "source": [
        "# SVM Poly classifier\n",
        "from sklearn import svm\n",
        "classifier = svm.SVC(kernel='rbf')\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe_svm_rbf = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe_svm_rbf.fit(X_train,y_train)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('cleaner', <__main__.predictors object at 0x7f7f6af4e6d8>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 2), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 t...ttern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function spacy_tokenizer at 0x7f7f6907e9d8>,\n",
              "                                 vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXgQpLZZIS1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy8J6bPtF1na",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6d0287c7-2332-4d37-bf0e-7cfd69461dca"
      },
      "source": [
        "#LOGISTIC\n",
        "from sklearn import metrics\n",
        "predicted = pipe_log.predict(X_test)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 0.765625\n",
            "Logistic Regression Precision: 0.7841483979763912\n",
            "Logistic Regression Recall: 0.8058925476603119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq2XTOJpMZyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f94907d1-e626-4991-b29b-69e40c4ed72b"
      },
      "source": [
        "#SVM\n",
        "from sklearn import metrics\n",
        "\n",
        "predicted = pipe_svm.predict(X_test)\n",
        "\n",
        "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "print(\"SVM Precision:\",metrics.precision_score(y_test, predicted))\n",
        "print(\"SVM Recall:\",metrics.recall_score(y_test, predicted))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Accuracy: 0.75\n",
            "SVM Precision: 0.7601296596434359\n",
            "SVM Recall: 0.8128249566724437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b5LSsiGLKCp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3e079895-3d5d-4899-dde6-4aa45d82a101"
      },
      "source": [
        "#SVM poly\n",
        "from sklearn import metrics\n",
        "\n",
        "predicted = pipe_svm_poly.predict(X_test)\n",
        "\n",
        "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "print(\"SVM Precision:\",metrics.precision_score(y_test, predicted))\n",
        "print(\"SVM Recall:\",metrics.recall_score(y_test, predicted))"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Accuracy: 0.6943359375\n",
            "SVM Precision: 0.671875\n",
            "SVM Recall: 0.8942807625649913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IGP2i82FJwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "89145751-33fa-4cd6-db75-ed4ad2894f14"
      },
      "source": [
        "\n",
        "print(pipe_log.predict([\" Enttäuscht\"]))\n",
        "print(pipe_svm.predict([\"Ist der Hype gerechtfertigt ?\"]))\n",
        "print(pipe_svm_rbf.predict([\"Ist der Hype gerechtfertigt ?\"]))\n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[0]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}